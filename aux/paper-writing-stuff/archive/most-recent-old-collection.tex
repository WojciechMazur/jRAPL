\documentclass{article}
\usepackage[margin=0.75in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[section]{placeins}
\usepackage{graphicx}
\graphicspath{{./images/}}
\usepackage{float}
\usepackage{listings}
\usepackage{framed}

\title{jRAPL Testing Results}
\author{Alejandro Servetto}
\begin{document}
\maketitle

System C is a Thinkpad E15. My new Computer (describe the rest of the specs...)

\section{UML Class Diagram}
    The UML is too big to fit on a page, but you can see it in the 'img' folder

\section{Async Monitor Comparison (C vs Java)}

\subsection{Explaining why Java was so much faster than C, or so it seems. It was based on the sleep implementation}
    \subsubsection{Sleep implementation}
        In Java, I use \texttt{Thread.sleep(msec)}. In C, I use the following:
        \begin{verbatim}
            int sleep_msec(long msec) {
	            struct timespec ts;
	            int res;

            	if (msec < 0) {
	            	errno = EINVAL;
            		return -1;
            	}

            	ts.tv_sec = msec / 1000;
	            ts.tv_nsec = (msec % 1000) * 1000000;

            	do {
	            	res = nanosleep(&ts, &ts);
	            } while (res && errno == EINTR);
	            
	            return res;
            }
        \end{verbatim}
At 0 ms, the C sleep is on average 52 usec slower than Java. At nonzero sleep rates, its different
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{AsyncMonitorCompares/sleep-timer_java-vs-c/c-java-avg-difference.png}
    	\caption{PKG dacapo results xalan}
    	\label{fig:xalan-fix-PKG}
    \end{figure}
    Comparing time for \{0,1,2,4,8,16,32,64\} milliseconds
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{AsyncMonitorCompares/sleep-timer_java-vs-c/time-compare-0.png}
    	\caption{recorded time to sleep 0 ms}
    	\label{fig:xalan-fix-PKG}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{AsyncMonitorCompares/sleep-timer_java-vs-c/time-compare-1.png}
    	\caption{recorded time to sleep 1 ms}
    	\label{fig:xalan-fix-PKG}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{AsyncMonitorCompares/sleep-timer_java-vs-c/time-compare-2.png}
    	\caption{recorded time to sleep 2 ms}
    	\label{fig:xalan-fix-PKG}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{AsyncMonitorCompares/sleep-timer_java-vs-c/time-compare-4.png}
    	\caption{recorded time to sleep 4 ms}
    	\label{fig:xalan-fix-PKG}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{AsyncMonitorCompares/sleep-timer_java-vs-c/time-compare-8.png}
    	\caption{recorded time to sleep 8 ms}
    	\label{fig:xalan-fix-PKG}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{AsyncMonitorCompares/sleep-timer_java-vs-c/time-compare-16.png}
    	\caption{recorded time to sleep 16 ms}
    	\label{fig:xalan-fix-PKG}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{AsyncMonitorCompares/sleep-timer_java-vs-c/time-compare-32.png}
    	\caption{recorded time to sleep 32 ms}
    	\label{fig:xalan-fix-PKG}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{AsyncMonitorCompares/sleep-timer_java-vs-c/time-compare-64.png}
    	\caption{recorded time to sleep 64 ms}
    	\label{fig:xalan-fix-PKG}
    \end{figure}


\subsection{Tied into Dacapo}
Made a callback harness, start a monitor right before a bench iteration, stop right after. Do 15 iterations, record data for the
latter 10. Experiments in this subsection were run on Jolteon

\subsubsection{Number of samples collected -- 1ms sampling rate}
Looks like C is actually faster than Java! Makes sense, given the JNI overhead difference we measured \textbf{NOTE THAT THIS IS ONLY THE AVERAGE OF 3 ITERS AFTER 1 WARMUP.}. I need to take a few hours to run it when I have the space to do so.

\textbf{Should I do samples / msec? The benchmarks each vary in runtime, which is why bars are longer between benchmarks, not necessarily about the benchmarks themselves}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/all-bench_numsamples.png}
    	\caption{Number of samples collected across all benchmarks}
    	\label{fig:xalan-fix-PKG}
    \end{figure}

\subsubsection{Number of samples collected -- 0 sampling rate, probably deprecated since the 0 rate isn't practical because of reasons}
Interestingly enough, Java version collected way more samples in around the same timespan. The plots below are averages
across 10 benchmark iterations. We see that this is consistent for Java across all the used benchmarks. Unless I'm mistaken we also see that the disparity between C versions and Java versions increases the longer the monitor runs (I should make a numeric way to quanitfy
the relationship between disparity and lifetime or something, I'm only going based off of eyeballing the graphs right now.

I intend to see if this is because of the timer implementation different or because of thread scheduling in C vs Java. Was suggested I
look at \texttt{strace} to see if threads behave differently. I started a thing to test the difference between the two timer implementations, although I need to flesh that out better

    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/avrora_numsamples.png}
    	\caption{results for avrora}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/batik_numsamples.png}
    	\caption{results for batik}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/biojava_numsamples.png}
    	\caption{results for biojava}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/cassandra_numsamples.png}
    	\caption{results for cassandra}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/eclipse_numsamples.png}
    	\caption{results for eclipse}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/fop_numsamples.png}
    	\caption{results for fop}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/graphchi_numsamples.png}
    	\caption{results for graphchi}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/h2_numsamples.png}
    	\caption{results for h2}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/h2o_numsamples.png}
    	\caption{results for h2o}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/jme_numsamples.png}
    	\caption{results for jme}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/jython_numsamples.png}
    	\caption{results for jython}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/kafka_numsamples.png}
    	\caption{results for luindex}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/lusearch_numsamples.png}
    	\caption{results for lusearch}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/pmd_numsamples.png}
    	\caption{results for pmd}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/sunflow_numsamples.png}
    	\caption{results for sunflow}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/tomcat_numsamples.png}
    	\caption{results for tomcat}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/tradebeans_numsamples.png}
    	\caption{results for tradebeans}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/tradesoap_numsamples.png}
    	\caption{results for tradesoap}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/xalan_numsamples.png}
    	\caption{results for xalan}
    \end{figure}
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=20cm,keepaspectratio]{jolteon/compare-async-monitor/dacapo/num-samples/zxing_numsamples.png}
    	\caption{results for zxing}
    \end{figure}

\subsection{naive approach}
Pretty unsophisticated way of comparing the C-Side and Java-Side Async Monitors. I have a driver that runs it
by keeping the main thread alive with Thread.sleep(). You can decide the sampling rate for the monitor. For these
results, I have 5000 ms lifetime with a 0 ms delay. These are just starters, though

Two performance metrics measured:
\begin{enumerate}
    \item Zero interval - how many consecutive energy samples there are with zero difference. Meaning the MSR didn't update
    \item Average energy per nonzero sample
\end{enumerate}

This is interesting...looks like the C-side thing is worse at sampling? I used the dynamic array based implementation,
not the linked list based implementation. Like I said above, the C and the Java version both had 5000 ms lifetime
and 0 ms delay. Could be because I start the dynamic array at a small size so it needs to realloc too much? Could
be good to preallocate based off of an estimate for how long the thing will be alive...
\begin{itemize}
    \item \textbf{Total C readings:}	77682
    \item \textbf{Total Java readings:}	875008
\end{itemize}


\subsubsection{Zero Intervals}
    Looks like the C side monitor has smaller zero intervals, so that means that it gets readings less frequently?
    These are histograms with 10 bins.

    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=17cm,keepaspectratio]{AsyncMonitorCompares/zero-intervals.png}
    	\caption{"Zero Intervals" histogram for each power domain, compared AsyncEnergyMonitorCSide and AsyncEnergyMonitorJavaSide}
    	\label{fig:xalan-PKG-Time-scatter}
    \end{figure}

\subsubsection{Average Sample Energy}
    Average nonzero energy sample for the CSide monitor and the JavaSide monitor. Looks like the C side monitor
    has higher energy samples, probably because C sampling has lower overhead??

    \begin{figure}[H]
    	\centering
    	\includegraphics[width=17cm,height=17cm,keepaspectratio]{AsyncMonitorCompares/avg-nonzero-energy-read.png}
    	\caption{Avg nonzero energy read in one sample per each power domain, compared AsyncEnergyMonitorCSide and AsyncEnergyMonitorJavaSide}
    	\label{fig:xalan-PKG-Time-scatter}
    \end{figure}
    
\section{All of my Async Monitor - Related Code}
    My intention behind the code is that you'd use an object of the abstract class AsyncEnergyMonitor, and initialize it to be one of the sub-classes, CSide and JavaSide. So you'd do \texttt{AsyncEnergyMonitor m = new AsyncEnergyMonitorJavaSide()} or \texttt{AsyncEnergyMonitor m = new AsyncEnergyMonitorCSide()}. All of the core utilities
    are defined or declared in the top-level abstract class, so you can use them "interchangeably" with no difference after they've been declared. You can see this
    in the sample main() driver for AsyncEnergyMonitor.java. It picks whether to initialize the AsyncEnergyMonitor as a CSide or JavaSide object based on command line arguments, and then
    the rest of the main driver's functionality works regardless of what kind it was instantiated as.
    
\subsection{AsyncEnergyMonitor.java}
    \begin{framed}
        \lstset{language=java}
        \lstinputlisting[tabsize=2]{java-src/AsyncEnergyMonitor.java}
    \end{framed}
\subsection{AsyncEnergyMonitorJavaSide.java}
    \begin{framed}
        \lstset{language=java}
        \lstinputlisting[tabsize=2]{java-src/AsyncEnergyMonitorJavaSide.java}
    \end{framed}
\subsection{AsyncEnergyMonitorCSide.java}
    \begin{framed}
        \lstset{language=java}
        \lstinputlisting[tabsize=2]{java-src/AsyncEnergyMonitorCSide.java}
    \end{framed}
\subsection{All of the C Side Code Related to AsyncEnergyMonitorCSide}
    \subsubsection{JNI Wrapper Calls}
        \begin{framed}
            \lstset{language=C}
            \lstinputlisting[tabsize=2]{native-src/jni-exports/AsyncEnergyMonitor.c}
        \end{framed}
    \subsubsection{AsyncEnergyMonitor.h}
        \begin{framed}
            \lstset{language=C}
            \lstinputlisting[tabsize=2]{native-src/just-c/AsyncEnergyMonitor.h}
        \end{framed}
    \subsubsection{AsyncEnergyMonitor.c}
        \begin{framed}
            \lstset{language=C}
            \lstinputlisting[tabsize=2]{native-src/just-c/AsyncEnergyMonitor.c}
        \end{framed}
    Different CSide Data Storage Methods for the AsyncEnergyMonitor
    \begin{itemize}
        \item CSideDataStorage.h
            \begin{framed}
                \lstset{language=C}
                \lstinputlisting[tabsize=2]{native-src/just-c/CSideDataStorage.h}
            \end{framed}
        \item CSideDataStorage.c
            \begin{framed}
                \lstset{language=C}
                \lstinputlisting[tabsize=2]{native-src/just-c/CSideDataStorage.c}
            \end{framed}
    \end{itemize}
    

\section{Memory Footprint}
\subsection{Dacapo runs}
Separate tables because we don't really need to compare across all benchmarks, right? Just for different implementations
within each benchmark.

These results are so inconsistent. I don't know what to make of them. Maybe the memory monitor is bad? Mention the (very
occasional) negative values.

Results are aggregate data for all iterations, then subtract \texttt{(monitoring memory with jRAPL) - 
(monitoring memory without jRAPL)} and find the \% difference

\subsubsection{avrora} xxx
    \input{MemoryFootprint/compare-footprint-benchmarks/avrora_memory-comparison}
\subsubsection{fop} xxx
    \input{MemoryFootprint/compare-footprint-benchmarks/fop_memory-comparison}
\subsubsection{batik} xxx
    \input{MemoryFootprint/compare-footprint-benchmarks/batik_memory-comparison}
\subsubsection{jython} xxx
    \input{MemoryFootprint/compare-footprint-benchmarks/jython_memory-comparison}
\subsubsection{luindex} xxx
    \input{MemoryFootprint/compare-footprint-benchmarks/luindex_memory-comparison}
\subsubsection{xalan} xxx
    \input{MemoryFootprint/compare-footprint-benchmarks/xalan_memory-comparison}
\subsubsection{h2} xx
    \input{MemoryFootprint/compare-footprint-benchmarks/h2_memory-comparison}
\subsubsection{lusearch} xx
    \input{MemoryFootprint/compare-footprint-benchmarks/lusearch_memory-comparison}
\subsubsection{zxing} xx
    \input{MemoryFootprint/compare-footprint-benchmarks/zxing_memory-comparison}
\subsubsection{h2o} xx
    \input{MemoryFootprint/compare-footprint-benchmarks/h2o_memory-comparison}
\subsubsection{eclipse} xx
    \input{MemoryFootprint/compare-footprint-benchmarks/eclipse_memory-comparison}
\subsubsection{tradebeans} xx
    \input{MemoryFootprint/compare-footprint-benchmarks/tradebeans_memory-comparison}
\subsubsection{tomcat} x
    \input{MemoryFootprint/compare-footprint-benchmarks/tomcat_memory-comparison}
\subsubsection{cassandra} x
    \input{MemoryFootprint/compare-footprint-benchmarks/cassandra_memory-comparison}
\subsubsection{sunflow} x
    \input{MemoryFootprint/compare-footprint-benchmarks/sunflow_memory-comparison}
\subsubsection{biojava} x
    \input{MemoryFootprint/compare-footprint-benchmarks/biojava_memory-comparison}
\subsubsection{pmd} x
    \input{MemoryFootprint/compare-footprint-benchmarks/pmd_memory-comparison}
\subsubsection{jme} x
    \input{MemoryFootprint/compare-footprint-benchmarks/jme_memory-comparison}
\subsubsection{graphchi} x
    \input{MemoryFootprint/compare-footprint-benchmarks/graphchi_memory-comparison}
\subsubsection{tradesoap} x
    \input{MemoryFootprint/compare-footprint-benchmarks/tradesoap_memory-comparison}
    
\subsection{Object Sample vs Primitive Array Sample}
Used the classmexer agent provided at https://javamex.com/classmexer/

It uses instrumentation to get the memory usage of an object, so the object header
(12 bytes) and the size of all of its fields. I made sure to ask for deep memory
usage, so if its fields are non-primitive types, we also get the number of bytes
in the entire object, not just of the reference stored in the top level object. Java
object sizes are padded up to the next multiple of 8. Units are bytes.

Below is the output of my test program, where I took an array sample and an object
sample, and got the deep memory use of each.
\begin{verbatim}










For raw stamp sample:
  primitive sample: 48 bytes
  object sample: 96 bytes
    [1164.4618, 184.5864, 4048.8065, 11751.1702]
    1164.4625,184.5864,4048.8231,11751.1893,1619625664691708

For diff sample (over a 100ms delay):
  primitive sample: 48 bytes
  object sample: 96 bytes
    [0.017399999999952342, 0.0, 0.16520000000036816, 0.30240000000048894]
    0.0235,0.0096,0.0237,0.1486,101030
    
    
    
\end{verbatim}

These results are on my computer, since Jolteon currently has my other experiments
running, so we'd get other values since it's a 2socket/3power-domain machine, as opposed to my 1socket/4power-domain machine.

Math checks out. Primitive sample is header (12 bytes) + 4 doubles (32 bytes) =
44, rounded up to 48.
Object sample is header (12) + primitive sample (44 +4padding) + Instant(12 + 8 + 8) = 88, round up to 96.

\textbf{66\% percent difference} in sizes of these implementations

\subsection{deprecated experiment}
    The only memory footprint test I did was seeing if there was a remarkable difference between the AsyncEnergyMonitor
    storing double[]s as energy samples or EnergyStats objects. However, Timur and I decided that it actually made
    the most sense to just store raw energy Strings returned directly from the native interface. There's no overhead
    that we'd have to deal with for parsing it while we're monitoring, and it may be that these end up getting written
    to a file anyways, so we'd have to turn the back into Strings. So instead, I just made some functionality to ask
    for samples as Objects or Arrays if we actually need to take them out of the AsyncEnergyMonitor during the code. But
    internally they're just strings. So the point of this specific experiment is over. Here are the results, though.

    What I did was ran two versions of the AsyncEnergyMonitorJavaSide, one which internally stored double[]s for
    energy samples, the other one which stored EnergyStats objects. Then I used an AsyncMemoryMonitor that I made.
    The memory monitor is the same type of thing as the energy monitor. Start a thread, collect memory samples at a
    set millisecond sampling rate. The memory monitor collected throughout the lifetime of the energymonitors as
    they grew in memory while collecting.
    
    I still have the memory monitor for other purposes, though.

    \begin{figure}[H]
    	\centering
    	\includegraphics[width=10cm,height=10cm,keepaspectratio]{MemoryFootprint/ArrayVsObjectStorage_(deprecated)/average_memory_consumption.png}
    	\caption{Memory growth of the Object storage based and Array storage based (now deprecated) implementations
    	of AsyncEnergyMonitorJavaSide}
    	\label{fig:avg-sample-PKG}
    \end{figure}
    
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=10cm,height=10cm,keepaspectratio]{MemoryFootprint/ArrayVsObjectStorage_(deprecated)/memory_consumption_table.png}
    	\caption{Some stats about the memory footprint of each implementation. These units are in bytes}
    	\label{fig:avg-sample-PKG}
    \end{figure}

    The object sample implementation was about .3 MB larger than the array based one, after running and growing
    for 20 seconds.


\section{Jolteon Stuff}
    All of these tests are done on the Jolteon server. Jolteon supports DRAM, CORE, and PKG, but \emph{not} GPU

    \subsection{JNI Overhead (difference between JNI calls timed from Java and from C)}
    These were done on Jolteon server with Java Microbenchmark Harness (JMH).
    
    \textbf{JMH Parameters:} 10 second trials, ran as many times as possible in the 10 second period. 1 fork, 5 warmup iterations, 10 measurement iterations. Results are cumulative (not averaged) for the 10 iterations, not including the warmups.
    
    We used our own timestamping as opposed to JMH's \texttt{Mode.AverageTime} utility because 1) we wanted individual readings to plot all of them as opposed to just the average and 2) we had to timestamp stuff on the C side, which there was no way JMH could do for us. Java timestamps were \texttt{Instant.now()} and C timestamps where \texttt{gettimeofday()}. Runtimes were stored in a \texttt{HashMap<Long,Long>} as a histogram. All measurements are in microseconds unless otherwise specified. 
    
    The graphs below are histograms. X axis is the microsecond runtimes, Y axis is how many readings we got
    for that runtime. Crazy high values (more than 3 standard deviations from the mean) are removed because they are relatively few and that stretch the graph and make it illegible. But we still captured
    them in the Python script, because they're probably important information and intend to render them somehow.

    The Java runtimes for EnergyStatCheck are way higher and more volatile...this is weird because when we were doing our original non-JMH runtime drivers,
    there was a pretty negligible difference between CSide and JavaSide runtime (see the first graph in Section 2, Runtime Tests). I don't know if it's because we wrote the JavaSide benchmarks poorly, but I couldn't find anything egregiously wrong. Maybe it's the magic of JMH giving us an actual look at what a real environment would be, and the original drivers we wrote were just prone to unrealistic optimization? Maybe it's because a String is actually being brought across the Java Native Interface; ProfileInit and ProfileDealloc are void functions. So the JVM could've realized that we weren't actually using the String and optimized out the conversion from a C JNIEXPORT String to a Java String. I'm not sure though, definitely something to look at more.
    
    
    \begin{figure}[H]
	    \centering
	    \includegraphics[width=10cm,height=10cm,keepaspectratio]{jolteon/jmh/jni-overhead/CSide_EnergyStatCheck_scatter.png}
	    \caption{Runtime for EnergyStatCheck on CSide}
	    \label{fig:jolteon-jmh-runtime-energystatcheck-c}
    \end{figure}
    \begin{figure}[H]
	    \centering
	    \includegraphics[width=10cm,height=10cm,keepaspectratio]{jolteon/jmh/jni-overhead/JavaSide_EnergyStatCheck_scatter.png}
	    \caption{Runtime for EnergyStatCheck on JavaSide}
	    \label{fig:jolteon-jmh-runtime-energystatcheck-java}
    \end{figure}
    
    \begin{figure}[H]
	    \centering
	    \includegraphics[width=10cm,height=10cm,keepaspectratio]{jolteon/jmh/jni-overhead/CSide_ProfileInit_scatter.png}
	    \caption{Runtime for ProfileInit on CSide}
	    \label{fig:jolteon-jmh-runtime-profileinit-c}
    \end{figure}
    \begin{figure}[H]
	    \centering
	    \includegraphics[width=10cm,height=10cm,keepaspectratio]{jolteon/jmh/jni-overhead/JavaSide_ProfileInit_scatter.png}
	    \caption{Runtime for ProfileInit on JavaSide}
	    \label{fig:jolteon-jmh-runtime-profileinit-java}
    \end{figure}

    \begin{figure}[H]
	    \centering
	    \includegraphics[width=10cm,height=10cm,keepaspectratio]{jolteon/jmh/jni-overhead/CSide_ProfileDealloc_scatter.png}
	    \caption{Runtime for ProfileDealloc on CSide}
	    \label{fig:jolteon-jmh-runtime-profileDealloc-c}
    \end{figure}
    \begin{figure}[H]
	    \centering
	    \includegraphics[width=10cm,height=10cm,keepaspectratio]{jolteon/jmh/jni-overhead/JavaSide_ProfileDealloc_scatter.png}
	    \caption{Runtime for ProfileDealloc on JavaSide}
	    \label{fig:jolteon-jmh-runtime-profileDealloc-java}
    \end{figure}
    

    \subsection{Time to read one MSR register}
    How long it takes to read one individual MSR register to get the RAPL counter. It's worth noting that we're not actually accessing the register, but a type of file that Linux provides as an interface where every 8 bytes internally maps to the actual hardware register. Just good to keep that distinction in mind.
    
    It's all hovering around one microsecond, with occasional relatively-high values. Values greater than 3 standard deviations weren't plotted because it makes the graph illegible, but we still captured them
    in the Python script and intend to represent them somehow, because it's still probably good to know
    that we'll very occasionally get super high runtimes.
    
    We used our own timestamping as opposed to JMH's \texttt{Mode.AverageTime} utility because 1) we wanted individual readings to plot all of them as opposed to just the average and 2) we had to timestamp stuff on the C side, which there was no way JMH could do for us. Java timestamps were \texttt{Instant.now()} and C timestamps where \texttt{gettimeofday()}. Runtimes were stored in a \texttt{HashMap<Long,Long>} as a histogram. All measurements are in microseconds unless otherwise specified. 
    
    Graphs below are histograms. X axis is the runtime, Y axis is how many readings we found at that runtime.
    
    \begin{figure}[H]
	    \centering
	    \includegraphics[width=10cm,height=10cm,keepaspectratio]{jolteon/jmh/readmsr-runtime/readMSR_CORE_scatter.png}
	    \caption{Runtime to read MSR for CORE RAPL counter}
	    \label{fig:CORE-rapl-counter}
    \end{figure}
    
    \begin{figure}[H]
	    \centering
	    \includegraphics[width=10cm,height=10cm,keepaspectratio]{jolteon/jmh/readmsr-runtime/readMSR_DRAM_scatter.png}
	    \caption{Runtime to read MSR for DRAM RAPL counter}
	    \label{fig:DRAM-rapl-counter}
    \end{figure}
    
    \begin{figure}[H]
	    \centering
	    \includegraphics[width=10cm,height=10cm,keepaspectratio]{jolteon/jmh/readmsr-runtime/readMSR_PKG_scatter.png}
	    \caption{Runtime to read MSR for PKG RAPL counter}
	    \label{fig:PKG-rapl-counter}
    \end{figure}
    
    %\begin{figure}[H]
	%    \centering
	%    \includegraphics[width=10cm,height=10cm,keepaspectratio]{jolteon/jmh/readmsr-runtime/readMSR_GPU_scatter.png}
	%    \caption{Runtime to read MSR for GPU RAPL counter}
	%    \label{fig:GPU-rapl-counter}
    %\end{figure}
    

\end{document}



























